{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3fb5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhoha\\AppData\\Local\\Temp\\ipykernel_14248\\2960264809.py:21: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"lxml\").text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>@AmericanAir right on cue with the delaysðŸ‘Œ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13651 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_clean  sentiment\n",
       "0                    @VirginAmerica What @dhepburn said.          0\n",
       "2      @VirginAmerica I didn't today... Must mean I n...          0\n",
       "3      @VirginAmerica it's really aggressive to blast...          1\n",
       "4      @VirginAmerica and it's a really big bad thing...          1\n",
       "5      @VirginAmerica seriously would pay $30 a fligh...          1\n",
       "...                                                  ...        ...\n",
       "14634         @AmericanAir right on cue with the delaysðŸ‘Œ          1\n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...          1\n",
       "14637  @AmericanAir Please bring American Airlines to...          0\n",
       "14638  @AmericanAir you have my money, you change my ...          1\n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...          0\n",
       "\n",
       "[13651 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#3\n",
    "data_import = pd.read_csv(\"Tweets.csv\")\n",
    "data = data_import.copy() #I'm working with a copy of the dataset (to avoid modifying the original dataset)\n",
    "\n",
    "#4\n",
    "data = data[data['airline_sentiment_confidence'] > 0.65]\n",
    "\n",
    "#5\n",
    "data['sentiment'] = data['airline_sentiment'].map({'neutral': 0, 'positive': 0, 'negative': 1})\n",
    "\n",
    "#6\n",
    "#In the column 'text', we can find some tags like (amp;) that belong to html error avoidance. They need to be removed! How? BeautifulSoup.\n",
    "def clean(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text \n",
    "#parser = analyseur: ywadha7 html/xml doc ki yabda fyh text+tags so that Python makes sense of it.\n",
    "#lxml is a parser that supports both html and xml docs => flexibility \n",
    "data['text_clean'] = data['text'].apply(clean)\n",
    "\n",
    "#7\n",
    "data = data.loc[:,['text_clean', 'sentiment']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6526ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'what', 'now', 'up', 'those', 'him', \"needn't\", 'a', 'ain', \"that'll\", \"should've\", 'yourselves', 'their', 'does', 'no', 'then', \"hasn't\", 'these', 'only', \"you'd\", \"haven't\", 'under', 'just', 'be', 'nor', 'you', \"don't\", \"shan't\", \"it's\", 'below', 're', 'by', 'while', 'needn', 'any', 'not', 'off', 'ourselves', 'here', 'which', 'doing', 'once', \"doesn't\", 'yours', 'i', 'the', 'mustn', 'has', 'hasn', 'mightn', 'than', 'out', 'how', 'he', 'did', 'same', \"she's\", 'through', \"isn't\", 'to', 'its', 'themselves', \"wasn't\", 'don', 'wasn', \"you're\", 'down', 'her', 'haven', 'd', 'all', \"couldn't\", 'hadn', 'was', \"you'll\", 'few', \"hadn't\", 'during', 'were', 'as', 'do', 'that', 'have', \"aren't\", 'more', 'are', 'y', 'ours', 'shan', 'in', 'his', 'if', 'can', 'both', 'itself', 'for', 'should', 'doesn', 'isn', 'and', 'will', 'with', 'll', 'having', 'couldn', 'where', 've', 'between', 'this', 's', 'because', 'o', 'shouldn', 'hers', 'myself', \"won't\", 'such', 'who', 'herself', 'each', 'our', \"wouldn't\", 'theirs', 'into', 'above', 'been', 'on', 'after', 'before', 'from', 'being', \"mightn't\", 'at', 'again', 'weren', 'your', 't', 'there', 'it', 'until', 'over', \"shouldn't\", 'didn', \"mustn't\", 'or', 'most', 'when', 'own', 'about', \"weren't\", 'they', 'my', 'so', 'she', 'am', 'wouldn', 'yourself', 'too', 'we', 'of', 'aren', 'why', 'had', 'against', 'very', 'is', 'some', \"didn't\", 'but', 'other', 'ma', 'them', 'an', 'me', \"you've\", 'himself', 'won', 'm', 'whom', 'further'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 71\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     62\u001b[0m ppln \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[0;32m     63\u001b[0m     vzr,\n\u001b[0;32m     64\u001b[0m     SVC(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m ppln\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    371\u001b[0m     cloned_transformer,\n\u001b[0;32m    372\u001b[0m     X,\n\u001b[0;32m    373\u001b[0m     y,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    376\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    640\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    641\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'what', 'now', 'up', 'those', 'him', \"needn't\", 'a', 'ain', \"that'll\", \"should've\", 'yourselves', 'their', 'does', 'no', 'then', \"hasn't\", 'these', 'only', \"you'd\", \"haven't\", 'under', 'just', 'be', 'nor', 'you', \"don't\", \"shan't\", \"it's\", 'below', 're', 'by', 'while', 'needn', 'any', 'not', 'off', 'ourselves', 'here', 'which', 'doing', 'once', \"doesn't\", 'yours', 'i', 'the', 'mustn', 'has', 'hasn', 'mightn', 'than', 'out', 'how', 'he', 'did', 'same', \"she's\", 'through', \"isn't\", 'to', 'its', 'themselves', \"wasn't\", 'don', 'wasn', \"you're\", 'down', 'her', 'haven', 'd', 'all', \"couldn't\", 'hadn', 'was', \"you'll\", 'few', \"hadn't\", 'during', 'were', 'as', 'do', 'that', 'have', \"aren't\", 'more', 'are', 'y', 'ours', 'shan', 'in', 'his', 'if', 'can', 'both', 'itself', 'for', 'should', 'doesn', 'isn', 'and', 'will', 'with', 'll', 'having', 'couldn', 'where', 've', 'between', 'this', 's', 'because', 'o', 'shouldn', 'hers', 'myself', \"won't\", 'such', 'who', 'herself', 'each', 'our', \"wouldn't\", 'theirs', 'into', 'above', 'been', 'on', 'after', 'before', 'from', 'being', \"mightn't\", 'at', 'again', 'weren', 'your', 't', 'there', 'it', 'until', 'over', \"shouldn't\", 'didn', \"mustn't\", 'or', 'most', 'when', 'own', 'about', \"weren't\", 'they', 'my', 'so', 'she', 'am', 'wouldn', 'yourself', 'too', 'we', 'of', 'aren', 'why', 'had', 'against', 'very', 'is', 'some', \"didn't\", 'but', 'other', 'ma', 'them', 'an', 'me', \"you've\", 'himself', 'won', 'm', 'whom', 'further'} instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#1\n",
    "train, test = train_test_split(\n",
    "    data, \n",
    "    test_size=0.2, \n",
    "    random_state=1)\n",
    "\n",
    "#2\n",
    "X_train = data['text_clean'].values\n",
    "X_test = data['text_clean'].values\n",
    "y_train = data['sentiment']\n",
    "y_test = data['sentiment']\n",
    "\n",
    "#3\n",
    "import nltk #nltk is a library specialized in working with human language data\n",
    "from nltk.tokenize import TweetTokenizer #this module helps splitting the text into words\n",
    "from nltk.stem import PorterStemmer #PorterStemmer to reduce words to their root form (it helps with the performance of the algorithme)\n",
    "\n",
    "\n",
    "#tokenization function\n",
    "def tkn(txt):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    return tokens \n",
    "#stemming function\n",
    "def stm(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tkn(word)]\n",
    "\n",
    "#4\n",
    "#stopwords are words with little value to the accuracy of the algorithm\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "ESW = set(stopwords.words('english'))\n",
    "\n",
    "#5\n",
    "from sklearn.feature_extraction.text import CountVectorizer #this library helps transforming our data into a numerical representation!\n",
    "#numerical conversion is crucial for ML algorithm\n",
    "\n",
    "vzr = CountVectorizer(\n",
    "    analyzer = 'word',#word level tokenization\n",
    "    tokenizer = tkn,\n",
    "    lowercase = True,\n",
    "    ngram_range = (1, 1),\n",
    "    stop_words = ESW\n",
    ")\n",
    "\n",
    "#6\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "SKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#Using StratifiedKFold assure maintaining the same distribution of classes in each Fold (for more relevant training)\n",
    "\n",
    "#7\n",
    "np.random.seed(1) #ensures getting the same result with each code execution\n",
    "\n",
    "#8\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ppln = make_pipeline(\n",
    "    vzr,\n",
    "    SVC(\n",
    "        probability=True,\n",
    "        kernel=\"linear\",\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    ")\n",
    "\n",
    "ppln.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c67b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
